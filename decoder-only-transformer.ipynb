{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0737e8c4cc834a0da6432dc2c83de4f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da5061d8f0b74e3389600b8f910d6ee8",
              "IPY_MODEL_f256ba6f2b2845eca6807052cc6773aa",
              "IPY_MODEL_7b03feed233a4debb8817b22155b4150"
            ],
            "layout": "IPY_MODEL_5d85ab24875d4f6a94ff7d0f7b7b7668"
          }
        },
        "da5061d8f0b74e3389600b8f910d6ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df689f227cfe4b559698ff6a10a831ff",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7bb0a3cae6ef4f1abebad27d8420a6e9",
            "value": "Epochâ€‡1/1:â€‡100%"
          }
        },
        "f256ba6f2b2845eca6807052cc6773aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5d4642c15724f30a84ee3dde22c2138",
            "max": 15116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d67b27dfa3048c799e687e92969d344",
            "value": 15116
          }
        },
        "7b03feed233a4debb8817b22155b4150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_643e74903dad415c87df4f6125406edf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9dcb298fdd5348dab59a1ce0ffc4340c",
            "value": "â€‡15116/15116â€‡[1:37:36&lt;00:00,â€‡â€‡2.58it/s,â€‡loss=0.0204]"
          }
        },
        "5d85ab24875d4f6a94ff7d0f7b7b7668": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "df689f227cfe4b559698ff6a10a831ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bb0a3cae6ef4f1abebad27d8420a6e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5d4642c15724f30a84ee3dde22c2138": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d67b27dfa3048c799e687e92969d344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "643e74903dad415c87df4f6125406edf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dcb298fdd5348dab59a1ce0ffc4340c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "    print(\"GPU count:\", torch.cuda.device_count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo0fLI5LnC5B",
        "outputId": "de4b4660-7a08-4d72-ed29-901e9d32b240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU name: Tesla T4\n",
            "GPU count: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx49orQedxlE",
        "outputId": "4471b0b1-a61c-45ec-af48-2e482d4692fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "%pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8s7hkEOPvSVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the Data"
      ],
      "metadata": {
        "id": "clCZcdOJebU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "try:\n",
        "    import requests\n",
        "except Exception:\n",
        "    from urllib.request import urlopen\n",
        "    class _SimpleResponse:\n",
        "        def __init__(self, content):\n",
        "            self.content = content\n",
        "    class _RequestsShim:\n",
        "        @staticmethod\n",
        "        def get(url):\n",
        "            return _SimpleResponse(urlopen(url).read())\n",
        "    requests = _RequestsShim()\n",
        "\n",
        "DATASOURCE = {\n",
        "    \"frankenstein\": \"https://www.gutenberg.org/ebooks/84.txt.utf-8\",\n",
        "    \"memoirs_of_grant\": \"https://www.gutenberg.org/ebooks/4367.txt.utf-8\",\n",
        "}\n",
        "\n",
        "for filename, url in DATASOURCE.items():\n",
        "    if not os.path.exists(f\"{filename}.txt\"):\n",
        "        resp = requests.get(url)\n",
        "        with open(f\"{filename}.txt\",\"wb\") as f:\n",
        "            f.write(resp.content)"
      ],
      "metadata": {
        "id": "U1lcgphxdzpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_gutenberg(filename):\n",
        "\n",
        "    with open(f\"{filename}\",\"r\" , encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    start = text.find(\"*** START OF THE PROJECT GUTENBERG EBOOK\")\n",
        "    start = text.find(\"\\n\",start)+1\n",
        "    end = text.find(\"*** END OF THE PROJECT GUTENBERG EBOOK\")\n",
        "\n",
        "    text = text[start:end]\n",
        "\n",
        "    text =\"\\n\".join(line.strip() for line in text.split(\"\\n\") if line.strip())\n",
        "\n",
        "    return text\n",
        "\n",
        "def get_dataset_txt():\n",
        "    all_text = []\n",
        "\n",
        "    for filename in DATASOURCE:\n",
        "        text = preprocess_gutenberg(f\"{filename}.txt\")\n",
        "        all_text.append(text)\n",
        "\n",
        "    return all_text\n",
        "\n",
        "\n",
        "text = get_dataset_txt()"
      ],
      "metadata": {
        "id": "341r3PKLd2kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a Tokenizer"
      ],
      "metadata": {
        "id": "K84oqu9Jd8Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w61YrXHid5as",
        "outputId": "3749c8d0-7b3a-41db-f405-5849bb80efea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(tokenizers.models.BPE())\n",
        "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space = True)\n",
        "tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
        "\n",
        "trainer = tokenizers.trainers.BpeTrainer(\n",
        "    vocab_size=10000,\n",
        "    special_tokens=[\"[pad]\",\"[eos]\"],\n",
        "    show_progress=True,\n",
        ")\n",
        "\n",
        "text = get_dataset_txt()\n",
        "\n",
        "tokenizer.train_from_iterator(text,trainer=trainer)\n",
        "tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"[pad]\"),pad_token=\"[pad]\")\n",
        "\n",
        "# Save the trained tokenizer\n",
        "\n",
        "tokenizer.save(\"gutenberg_tokenizer.json\",pretty=True)"
      ],
      "metadata": {
        "id": "9QaydDoMd-Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"gutenberg_tokenizer.json\")"
      ],
      "metadata": {
        "id": "sHJBo_HNeBWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding"
      ],
      "metadata": {
        "id": "bEhck7A0eI0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7AhjeuUj6cL",
        "outputId": "a339dda7-5027-4fd6-c8f2-e60026c710d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x.chunk(2, dim=-1)\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(x, cos, sin):\n",
        "    return (x * cos) + (rotate_half(x) * sin)\n",
        "\n",
        "class RotaryPositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len=1024):\n",
        "        super().__init__()\n",
        "        N = 10000\n",
        "        inv_freq = 1.0 / (N ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        inv_freq = torch.cat((inv_freq, inv_freq), dim=-1)\n",
        "        position = torch.arange(max_seq_len).float()\n",
        "        sinusoid_inp = torch.outer(position, inv_freq)  # (max_seq_len, dim)\n",
        "        self.register_buffer(\"cos\", sinusoid_inp.cos())\n",
        "        self.register_buffer(\"sin\", sinusoid_inp.sin())\n",
        "\n",
        "    def forward(self, x, seq_len=None):\n",
        "        if seq_len is None:\n",
        "            seq_len = x.size(1)\n",
        "        cos = self.cos[:seq_len].view(1, seq_len, 1, -1)\n",
        "        sin = self.sin[:seq_len].view(1, seq_len, 1, -1)\n",
        "        return apply_rotary_pos_emb(x, cos, sin)\n",
        "\n",
        "sequence = torch.randn(1, 10, 4, 128)\n",
        "rope = RotaryPositionalEncoding(128)\n",
        "new_sequence = rope(sequence)\n",
        "\n"
      ],
      "metadata": {
        "id": "B6z68mCQePgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sequence"
      ],
      "metadata": {
        "id": "8k-BRmlpeR_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0bf395-078c-4ab2-e2b6-ed9bff347b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 1.3500e-01, -1.0934e+00,  7.2225e-01,  ..., -1.1666e-01,\n",
              "           -1.4374e+00,  1.2989e+00],\n",
              "          [-1.8225e+00,  3.0511e-01, -1.0743e+00,  ...,  1.3728e+00,\n",
              "            7.2227e-01, -4.4180e-01],\n",
              "          [-1.1341e-01,  7.6998e-01,  1.5093e-01,  ..., -9.9555e-01,\n",
              "           -5.5373e-01,  8.5772e-01],\n",
              "          [-5.7886e-01,  2.1247e-01, -7.5288e-01,  ..., -9.7738e-01,\n",
              "            1.8347e+00, -3.1595e-01]],\n",
              "\n",
              "         [[ 2.4295e-01, -5.6094e-01,  5.8951e-01,  ..., -1.0252e+00,\n",
              "            6.7968e-01,  1.2072e+00],\n",
              "          [-1.3974e-01,  1.2955e+00, -7.5753e-01,  ..., -1.8200e+00,\n",
              "            8.4931e-01,  8.6374e-01],\n",
              "          [-1.3631e-01,  7.3877e-01, -1.5310e+00,  ...,  1.2854e+00,\n",
              "            8.2701e-01, -8.3987e-01],\n",
              "          [-1.4309e+00,  7.8992e-01,  8.2725e-01,  ..., -1.7842e-01,\n",
              "            2.0855e-01,  5.3596e-01]],\n",
              "\n",
              "         [[ 1.7888e-01,  1.1067e+00, -1.4434e-01,  ...,  1.1007e+00,\n",
              "           -7.9047e-01, -4.5984e-01],\n",
              "          [-4.9294e-01, -8.4089e-01, -9.9837e-01,  ..., -4.4321e-01,\n",
              "           -5.3429e-01,  4.8312e-01],\n",
              "          [ 8.7419e-01, -1.0337e+00, -2.1190e+00,  ..., -4.1785e-01,\n",
              "            4.4781e-01,  6.7147e-01],\n",
              "          [ 1.0454e+00, -2.2532e+00,  1.8698e-01,  ..., -4.6640e-01,\n",
              "            6.0789e-01,  4.1917e-02]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-2.4307e-02, -2.4862e+00,  8.6624e-01,  ..., -4.1086e-01,\n",
              "           -3.8091e-01, -1.0912e-03],\n",
              "          [-7.9203e-01,  7.2647e-01, -5.4354e-01,  ...,  2.2942e-01,\n",
              "            3.4394e-01,  9.3628e-01],\n",
              "          [-1.0860e+00,  1.3126e-01,  1.5097e+00,  ...,  1.6864e-01,\n",
              "           -7.0964e-01,  2.8901e-01],\n",
              "          [-1.3753e-01,  1.4005e+00,  2.2146e+00,  ...,  3.1296e-01,\n",
              "           -1.0780e+00, -3.3261e-01]],\n",
              "\n",
              "         [[ 1.6864e+00,  3.4919e-01, -2.7265e-01,  ..., -2.6357e-01,\n",
              "           -6.3360e-01,  1.0672e+00],\n",
              "          [ 7.7034e-02,  1.5378e+00,  2.3252e-01,  ...,  2.5521e-02,\n",
              "            6.6358e-01, -1.8891e+00],\n",
              "          [-1.3691e+00,  1.6866e+00,  7.7614e-01,  ...,  3.5226e-01,\n",
              "            1.3768e+00, -6.7577e-01],\n",
              "          [-1.1190e+00, -2.2745e+00, -6.6841e-01,  ..., -2.2312e+00,\n",
              "           -3.8273e-01, -3.8104e-01]],\n",
              "\n",
              "         [[-1.7161e-02,  9.7204e-02, -8.6528e-01,  ...,  1.8541e-01,\n",
              "           -7.7738e-01,  1.7734e-01],\n",
              "          [-1.5824e-01,  5.7916e-02, -1.6571e-01,  ...,  4.2791e-01,\n",
              "            4.9824e-01,  1.6654e-01],\n",
              "          [ 1.8703e-01, -9.1082e-01, -4.0393e-01,  ...,  9.4147e-02,\n",
              "            1.0377e+00, -1.0918e+00],\n",
              "          [-8.3094e-01,  2.4747e-02, -8.1239e-01,  ..., -8.7643e-01,\n",
              "            1.4559e+00, -1.9214e+00]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grouped Query Attention"
      ],
      "metadata": {
        "id": "QxuREPi8ep8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "class GQA(nn.Module):\n",
        "  def __init__(self,hidden_dims,num_heads,num_kv_heads,drop_out=0.1):\n",
        "    super().__init__()\n",
        "    self.num_head = num_heads\n",
        "    self.num_kv_head = num_kv_heads\n",
        "    self.head_dim = hidden_dims // num_heads\n",
        "    self.num_group = num_heads // num_kv_heads\n",
        "    self.dropout = drop_out\n",
        "\n",
        "    self.q_proj = nn.Linear(hidden_dims,self.num_head*self.head_dim)\n",
        "    self.k_proj = nn.Linear(hidden_dims,self.num_kv_head*self.head_dim)\n",
        "    self.v_proj = nn.Linear(hidden_dims,self.num_kv_head*self.head_dim)\n",
        "    self.out_proj = nn.Linear(self.num_head*self.head_dim,hidden_dims)\n",
        "\n",
        "\n",
        "  def forward(self,q,k,v,mask=None,rope=None):\n",
        "    q_batch_size,q_seq_len,hidden_dim = q.shape\n",
        "    k_batch_size,k_seq_len,hidden_dim = k.shape\n",
        "    v_batch_size,v_seq_len,hidden_dim = v.shape\n",
        "\n",
        "    q = self.q_proj(q).view(q_batch_size,q_seq_len,-1,self.head_dim).transpose(1,2)\n",
        "    k = self.k_proj(k).view(k_batch_size,k_seq_len,-1,self.head_dim).transpose(1,2)\n",
        "    v = self.v_proj(v).view(v_batch_size,v_seq_len,-1,self.head_dim).transpose(1,2)\n",
        "\n",
        "    if rope:\n",
        "      q = rope(q)\n",
        "      k = rope(k)\n",
        "\n",
        "    q = q.contiguous()\n",
        "    k = k.contiguous()\n",
        "    v = v.contiguous()\n",
        "\n",
        "    output = F.scaled_dot_product_attention(q,k,v,attn_mask=mask,dropout_p=self.dropout,enable_gqa=True)\n",
        "\n",
        "    output = output.transpose(1,2).reshape(q_batch_size, q_seq_len, hidden_dim).contiguous()\n",
        "    output = self.out_proj(output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "UpeBZetE9C7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example\n",
        "gqa = GQA(hidden_dims=128, num_heads=8, num_kv_heads=2)\n",
        "\n",
        "q = torch.randn(2, 16, 128)\n",
        "k = torch.randn(2, 16, 128)\n",
        "v = torch.randn(2, 16, 128)\n",
        "\n",
        "out = gqa(q, k, v)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "phdMRMhxCfsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ca9c8c-819e-4d8f-bb2d-0e36ee01dc25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 16, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mask\n"
      ],
      "metadata": {
        "id": "D_NoMpWm-9Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_causal_mask(seq_len, device):\n",
        "    return torch.triu(\n",
        "        torch.ones((seq_len, seq_len), device=device, dtype=torch.bool),\n",
        "        diagonal=1\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Wh36V4-lCxd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixture of Expert Models"
      ],
      "metadata": {
        "id": "eYZcA4oE_Fk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "  def __init__(self,hidden_dim,intermediate_dim):\n",
        "    super().__init__()\n",
        "    self.get = nn.Linear(hidden_dim,intermediate_dim)\n",
        "    self.up = nn.Linear(hidden_dim,intermediate_dim)\n",
        "    self.down = nn.Linear(intermediate_dim,hidden_dim)\n",
        "    self.act = nn.SiLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.act(self.get(x))*self.up(x)\n",
        "    out = self.down(x)\n",
        "    return out"
      ],
      "metadata": {
        "id": "QUpYK1Dj_EAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SwiGLU(8,2) # example"
      ],
      "metadata": {
        "id": "mrBdoXvxk5GH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fab004b-5632-4f1e-dd4d-c9dc907422ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SwiGLU(\n",
              "  (get): Linear(in_features=8, out_features=2, bias=True)\n",
              "  (up): Linear(in_features=8, out_features=2, bias=True)\n",
              "  (down): Linear(in_features=2, out_features=8, bias=True)\n",
              "  (act): SiLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MoELayer(nn.Module):\n",
        "  def __init__(self,hidden_dim,intermediate_dim,moe_experts,top_k=2):\n",
        "    super().__init__()\n",
        "    self.experts = moe_experts\n",
        "    self.topk = top_k\n",
        "    self.expert =  nn.ModuleList([\n",
        "        SwiGLU(hidden_dim,intermediate_dim) for _ in range(moe_experts)\n",
        "    ])\n",
        "    self.roter = nn.Linear(hidden_dim,moe_experts)\n",
        "  def forward(self,hidden_states):\n",
        "    batch_size,seq_len,hidden_dim = hidden_states.shape\n",
        "\n",
        "    hidden_stated_reshaped = hidden_states.view(-1,hidden_dim)\n",
        "    router_logits = self.roter(hidden_stated_reshaped)\n",
        "\n",
        "    top_k_logits, top_k_indices = torch.topk(router_logits,self.topk,dim=-1)\n",
        "    top_k_prob = F.softmax(top_k_logits,dim=-1)\n",
        "\n",
        "    output = torch.zeros(batch_size*seq_len,hidden_dim,device=hidden_states.device,dtype=hidden_states.dtype)\n",
        "    unique_experts = torch.unique(top_k_indices)\n",
        "\n",
        "    for i in unique_experts:\n",
        "      expert_id = int(i)\n",
        "      mask = (top_k_indices == expert_id)\n",
        "      token_mask  = mask.any(dim=1)\n",
        "      assert token_mask.any(),f\"Expecting some tokens using expert {expert_id}\"\n",
        "\n",
        "      expert_input = hidden_stated_reshaped[token_mask]\n",
        "      expert_wight = top_k_prob[mask].unsqueeze(-1)\n",
        "      expert_output = self.expert[expert_id](expert_input)\n",
        "\n",
        "      output[token_mask] = expert_output*expert_wight\n",
        "\n",
        "    output = output.view(batch_size,seq_len,hidden_dim)\n",
        "    return output"
      ],
      "metadata": {
        "id": "f6eVDQbClIP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MoELayer(8,2,8)"
      ],
      "metadata": {
        "id": "bMqz16uU4TVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc1ed61-7408-482f-ea37-41d6664751d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MoELayer(\n",
              "  (expert): ModuleList(\n",
              "    (0-7): 8 x SwiGLU(\n",
              "      (get): Linear(in_features=8, out_features=2, bias=True)\n",
              "      (up): Linear(in_features=8, out_features=2, bias=True)\n",
              "      (down): Linear(in_features=2, out_features=8, bias=True)\n",
              "      (act): SiLU()\n",
              "    )\n",
              "  )\n",
              "  (roter): Linear(in_features=8, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMS Norm and Skip Connections"
      ],
      "metadata": {
        "id": "TPlQTf9jex-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def  __init__(self,hidden_dims,num_heads,num_kv_heads,moe_experts,moe_topk, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.self_atten = GQA(hidden_dims,num_heads,num_kv_heads,drop_out=dropout)\n",
        "    self.mlp = MoELayer(hidden_dims,4*hidden_dims,moe_experts,moe_topk)\n",
        "    self.norm1 = nn.RMSNorm(hidden_dims)\n",
        "    self.norm2 = nn.RMSNorm(hidden_dims)\n",
        "\n",
        "  def forward(self,x,mask=None,rope=None):\n",
        "    out = self.norm1(x)\n",
        "    out = self.self_atten(out,out,out,mask,rope)\n",
        "    x = x+out\n",
        "\n",
        "    out = self.norm2(x)\n",
        "    out = self.mlp(out)\n",
        "\n",
        "    return x + out"
      ],
      "metadata": {
        "id": "hvJhm3Gq6u4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Transformer Model"
      ],
      "metadata": {
        "id": "fG70jwr5e2N9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_config = {\n",
        "#     \"num_layers\": 8,\n",
        "#     \"num_heads\": 8,\n",
        "#     \"num_kv_heads\": 4,\n",
        "#     \"hidden_dim\": 768,\n",
        "#     \"moe_experts\": 8,\n",
        "#     \"moe_topk\": 2,\n",
        "#     \"max_seq_len\": 512,\n",
        "#     \"vocab_size\": len(tokenizer.get_vocab()),\n",
        "#     \"dropout\": 0.1,\n",
        "# }\n",
        "\n",
        "model_config = {\n",
        "    # -------------------------\n",
        "    # Reduced depth (BIG speed gain)\n",
        "    # -------------------------\n",
        "    \"num_layers\": 5,          # was 8\n",
        "\n",
        "    # -------------------------\n",
        "    # Attention (GQA preserved)\n",
        "    # -------------------------\n",
        "    \"num_heads\": 6,           # was 8\n",
        "    \"num_kv_heads\": 2,        # was 4\n",
        "\n",
        "    # -------------------------\n",
        "    # Reduced width (BIG memory gain)\n",
        "    # -------------------------\n",
        "    \"hidden_dim\": 384,        # was 768\n",
        "\n",
        "    # -------------------------\n",
        "    # MoE (kept but lighter)\n",
        "    # -------------------------\n",
        "    \"moe_experts\": 2,         # was 8\n",
        "    \"moe_topk\": 1,            # was 2 (faster + stable)\n",
        "\n",
        "    # -------------------------\n",
        "    # Sequence length (attention is O(N^2))\n",
        "    # -------------------------\n",
        "    \"max_seq_len\": 256,       # was 512\n",
        "\n",
        "    # -------------------------\n",
        "    # Vocabulary (unchanged)\n",
        "    # -------------------------\n",
        "    \"vocab_size\": len(tokenizer.get_vocab()),\n",
        "\n",
        "    # -------------------------\n",
        "    # Regularization\n",
        "    # -------------------------\n",
        "    \"dropout\": 0.1,\n",
        "}\n"
      ],
      "metadata": {
        "id": "BLfqrhvLCygO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config"
      ],
      "metadata": {
        "id": "lVTE7D5LNCiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0cccc39-76df-4854-925e-df361a98ca43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_layers': 5,\n",
              " 'num_heads': 6,\n",
              " 'num_kv_heads': 2,\n",
              " 'hidden_dim': 384,\n",
              " 'moe_experts': 2,\n",
              " 'moe_topk': 1,\n",
              " 'max_seq_len': 256,\n",
              " 'vocab_size': 10000,\n",
              " 'dropout': 0.1}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerationModel(nn.Module):\n",
        "  def __init__(self, num_layers, num_heads, num_kv_heads, hidden_dim,\n",
        "                 moe_experts, moe_topk, max_seq_len, vocab_size, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.rope = RotaryPositionalEncoding(hidden_dim // num_heads, max_seq_len)\n",
        "    self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "    self.decoder = nn.ModuleList([\n",
        "        Decoder(hidden_dim,num_heads,num_kv_heads,moe_experts,moe_topk,dropout)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "    self.norm = nn.RMSNorm(hidden_dim)\n",
        "    self.out = nn.Linear(hidden_dim,vocab_size)\n",
        "\n",
        "  def forward(self,ids,mask=None):\n",
        "    x = self.embedding(ids)\n",
        "    for decoder in self.decoder:\n",
        "      x = decoder(x,mask,self.rope)\n",
        "\n",
        "    x = self.norm(x)\n",
        "\n",
        "    return self.out(x)\n",
        "model = TextGenerationModel(**model_config)"
      ],
      "metadata": {
        "id": "clZNQghnNZSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hvi1SDrkTvVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54086d27-0a38-45fe-97cb-1602becb13e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextGenerationModel(\n",
              "  (rope): RotaryPositionalEncoding()\n",
              "  (embedding): Embedding(10000, 384)\n",
              "  (decoder): ModuleList(\n",
              "    (0-4): 5 x Decoder(\n",
              "      (self_atten): GQA(\n",
              "        (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (k_proj): Linear(in_features=384, out_features=128, bias=True)\n",
              "        (v_proj): Linear(in_features=384, out_features=128, bias=True)\n",
              "        (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (mlp): MoELayer(\n",
              "        (expert): ModuleList(\n",
              "          (0-1): 2 x SwiGLU(\n",
              "            (get): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (up): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (down): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (act): SiLU()\n",
              "          )\n",
              "        )\n",
              "        (roter): Linear(in_features=384, out_features=2, bias=True)\n",
              "      )\n",
              "      (norm1): RMSNorm((384,), eps=None, elementwise_affine=True)\n",
              "      (norm2): RMSNorm((384,), eps=None, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (norm): RMSNorm((384,), eps=None, elementwise_affine=True)\n",
              "  (out): Linear(in_features=384, out_features=10000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ],
      "metadata": {
        "id": "a4vUAR5KT_fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GutenbergDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,text, tokenizer, seq_len=512):\n",
        "      self.seq_len = seq_len\n",
        "      self.encoded = tokenizer.encode(text).ids\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.encoded)-self.seq_len\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "      chunk = self.encoded[idx:idx+self.seq_len+1]\n",
        "      x = torch.tensor(chunk[:-1])\n",
        "      y = torch.tensor(chunk[1:])\n",
        "      return x,y\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "text = \"\\n\".join(get_dataset_txt())\n",
        "dataset = GutenbergDataset(text,tokenizer,seq_len=model_config[\"max_seq_len\"])\n",
        "dataloader = torch.utils.data.DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True)"
      ],
      "metadata": {
        "id": "qbqtC2usTxh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device).to(torch.bfloat16)"
      ],
      "metadata": {
        "id": "M8dQXDeqbE1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "RwKqr_bX9tTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8924d2b5-10ee-4928-e4a6-af02d56c8e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from tqdm.auto import tqdm   # âœ… better than tqdm.notebook in Colab\n",
        "\n",
        "N_EPOCHS = 1\n",
        "LR = 5e-4\n",
        "WARMUP_STEPS = 1000\n",
        "CLIP_NORM = 1.0\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id(\"[pad]\"))\n",
        "\n",
        "# Learning rate scheduling\n",
        "warmup_scheduler = optim.lr_scheduler.LinearLR(\n",
        "    optimizer,\n",
        "    start_factor=0.01,\n",
        "    end_factor=1.0,\n",
        "    total_iters=WARMUP_STEPS\n",
        ")\n",
        "\n",
        "total_steps = N_EPOCHS * len(dataloader)\n",
        "cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=max(1, total_steps - WARMUP_STEPS),\n",
        "    eta_min=0.0\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.SequentialLR(\n",
        "    optimizer,\n",
        "    schedulers=[warmup_scheduler, cosine_scheduler],\n",
        "    milestones=[WARMUP_STEPS]\n",
        ")\n",
        "\n",
        "print(f\"Training for {N_EPOCHS} epochs with {len(dataloader)} steps per epoch\")\n",
        "\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        dataloader,\n",
        "        desc=f\"Epoch {epoch + 1}/{N_EPOCHS}\",\n",
        "        leave=True,\n",
        "        dynamic_ncols=True,\n",
        "        mininterval=0.5,     # ðŸ”´ force frequent refresh\n",
        "        smoothing=0.0        # ðŸ”´ disable averaging lag\n",
        "    )\n",
        "\n",
        "    for step, (x, y) in enumerate(progress_bar):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        mask = create_causal_mask(\n",
        "            seq_len=seq_len,\n",
        "            device=device\n",
        "        ).unsqueeze(0)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        outputs = model(x, mask)\n",
        "\n",
        "        loss = loss_fn(\n",
        "            outputs.view(-1, outputs.size(-1)),\n",
        "            y.view(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            model.parameters(),\n",
        "            CLIP_NORM\n",
        "        )\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # ðŸ”´ explicit refresh\n",
        "        progress_bar.set_postfix_str(f\"loss={loss.item():.4f}\")\n",
        "        progress_bar.refresh()\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        torch.save(model.state_dict(), \"textgen_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "0737e8c4cc834a0da6432dc2c83de4f9",
            "da5061d8f0b74e3389600b8f910d6ee8",
            "f256ba6f2b2845eca6807052cc6773aa",
            "7b03feed233a4debb8817b22155b4150",
            "5d85ab24875d4f6a94ff7d0f7b7b7668",
            "df689f227cfe4b559698ff6a10a831ff",
            "7bb0a3cae6ef4f1abebad27d8420a6e9",
            "e5d4642c15724f30a84ee3dde22c2138",
            "8d67b27dfa3048c799e687e92969d344",
            "643e74903dad415c87df4f6125406edf",
            "9dcb298fdd5348dab59a1ce0ffc4340c"
          ]
        },
        "id": "26bdlzltbQFr",
        "outputId": "64ec82f3-b3f8-464f-b9e5-0184234ff957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 1 epochs with 15116 steps per epoch\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/1:   0%|          | 0/15116 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0737e8c4cc834a0da6432dc2c83de4f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 | Avg Loss: 0.0290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Model"
      ],
      "metadata": {
        "id": "9RaN4W5oe9Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_length=120,\n",
        "    temperature=0.8,\n",
        "    top_k=40,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=2\n",
        "):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    input_ids = torch.tensor(\n",
        "        tokenizer.encode(prompt).ids,\n",
        "        dtype=torch.long\n",
        "    ).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            outputs = model(input_ids)\n",
        "\n",
        "            if isinstance(outputs, tuple):\n",
        "                logits = outputs[0][:, -1, :]\n",
        "            else:\n",
        "                logits = outputs[:, -1, :]\n",
        "\n",
        "            logits = (logits / temperature).clone()\n",
        "\n",
        "            # repetition penalty\n",
        "            for token_id in set(input_ids[0].tolist()):\n",
        "                logits[0, token_id] /= repetition_penalty\n",
        "\n",
        "            # top-k\n",
        "            if top_k > 0:\n",
        "                top_k_vals, _ = torch.topk(logits, top_k)\n",
        "                min_top_k = top_k_vals[:, -1].unsqueeze(-1)\n",
        "                logits = torch.where(\n",
        "                    logits < min_top_k,\n",
        "                    torch.full_like(logits, float(\"-inf\")),\n",
        "                    logits\n",
        "                )\n",
        "\n",
        "            # top-p (FIXED)\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(\n",
        "                F.softmax(sorted_logits, dim=-1), dim=-1\n",
        "            )\n",
        "\n",
        "            sorted_indices_to_remove = (cumulative_probs > top_p).clone()\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "\n",
        "            logits = logits.clone()\n",
        "            logits[:, indices_to_remove] = float(\"-inf\")\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "    return tokenizer.decode(input_ids[0].tolist())\n"
      ],
      "metadata": {
        "id": "obDrIU9ZEFWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompts = [\n",
        "    \"Long before the final outcome was known, the young officer began to question the cost of obedience,\",\n",
        "    \"Driven by an unrelenting desire to achieve greatness, he ignored the warnings that surrounded his work,\",\n",
        "    \"The expedition was planned with confidence, yet uncertainty followed every step forward,\",\n",
        "    \"Though trained in discipline and order, he found himself confronting chaos beyond preparation,\",\n",
        "    \"What began as an intellectual pursuit gradually transformed into an experiment with irreversible consequences,\",\n",
        "]"
      ],
      "metadata": {
        "id": "3P969pC5pV0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nGenerating sample texts:\")\n",
        "for prompt in test_prompts:\n",
        "    generated = generate_text(model, tokenizer, prompt)\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Generated: {generated}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3w7kLQJpYaW",
        "outputId": "542dc2ab-a2d1-4ae0-f14b-65f7344970fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating sample texts:\n",
            "\n",
            "Prompt: Long before the final outcome was known, the young officer began to question the cost of obedience,\n",
            "Generated:  Long before the final outcome was known, the young officer began to question the cost of obedience,ward running appointed later there crossed on points as destroyed engaged commenced stationed without during formed necessary very almost too north had stated learned expected drew enabled afterwards went guarding finally opened fired again stopped ordered Halleck showed described since conducted intrenched difficult came took received breaking reached looking started given ceased sent rendered changed pushed collected now confined commanded drove going occupied firing informed educated generally opposed notified supposed brought about Charleston got fallen repulsed impassable assaulted assigned together runs ready directly against detained reported advanced mounted possessed just deserted excited done thrown at driven sufficiently performed supported marched increased directed coming effected obliged struck utterly defeated somewhat proposed lost written due fortified compelled captured moved urged required heard\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: Driven by an unrelenting desire to achieve greatness, he ignored the warnings that surrounded his work,\n",
            "Generated:  Driven by an unrelenting desire to achieve greatness, he ignored the warnings that surrounded his work, upon me. The them what it and how!ial my svencelyyilise a her for art,â€\n",
            "day had taken their position'simmurements--am in Tcassorationsehu was alwaysment of large line moreons)'aryantions lay our other foot-beinSwanion as General Gettera man whofully or which arrived; night where these several years point.â€ I shalleraschous before Elizabeth? His hideous whom they were every than your careious,\" rality is Clerval became such you are expressed\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: The expedition was planned with confidence, yet uncertainty followed every step forward,\n",
            "Generated:  The expedition was planned with confidence, yet uncertainty followed every step forward, in New Orleans extended fixed directly made ceased ensued communication promptly consisted required always added apparently drawn closed guarded put worn unfortunate communicated displayed desired accomplished forwarded confronting able relieved prepared held lost got talked resulted asked designated Granger wanted given remained assigned passes continually thrown increased became organized created reached deceived examined let thus promoted lying informed cut written silent prevented understood becomes learned behind impressed sufficient surrounded stopped placed absent free produced filled seized deployed attacked struck retreated succeeded moved consented slow obliged mentioned detained proceeding become provided served done caused burned grown fought executed sprung taught busy begun for met working dressed suffered exhibited spared proposed assured sunk beneath shone charged threatened appears started set resorted opposed\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: Though trained in discipline and order, he found himself confronting chaos beyond preparation,\n",
            "Generated:  Though trained in discipline and order, he found himself confronting chaos beyond preparation, he thus behind Longstreet was wounded flank of Virginia did it except for does there,\" on an troops reached Schofield had sometimes withoutlet where Captain Kautzial engagement came promptly separated every advisable with commissary passes kept attacked I lost Richmond before again nor Warren effected when while Pillow crossed early quite between February Granger's Ferry fortified guarding destroying Thomas making Memphis during Mexico engaged formed opened then completely indicated drew commanding marked because besides Canby finally dividing Vicksburg whose 1863 Anna took extended arrived increased more directed Bragg declined such common), fell destroyed embraced finding as later ensued supposed remained after Charleston amidst laying possessed ready also assigned together here seized close worthy spent directly\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: What began as an intellectual pursuit gradually transformed into an experiment with irreversible consequences,\n",
            "Generated:  What began as an intellectual pursuit gradually transformed into an experiment with irreversible consequences, there,\" was appeared extended myselfination proved precaution ensued became assured died made sympathisaled stood resulted had observed useless gift broke showed idleendinationsake ignoranceressionarily learned embraced wandered proposed forth guarded again remembered explainedorableices now declined arose promptly heard retired confessianower displayedively lost endeavoured fully happenedhood thought interseuesowago referredless couple talracticularfer fulfilled usually desired forwardedfully approached insticolic forgotrationnanceinentorbertouring requested stopped understood Bruinsburg beneath suspense spoke consented beneatigarelyuman submitted departed affordassed seems caused Hardee avoided consistedained terrificpecayed containing wet graduated remained met Once shellsalth indicated arrangements\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UM17_Im45CeD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}